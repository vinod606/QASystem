{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YJjwf2kiRKn"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate evaluate rouge_score\n",
        "!pip install transformers accelerate evaluate rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install meteor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTENx-eQrdlu"
      },
      "outputs": [],
      "source": [
        "# pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9aybkig_0MYL"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init(project=\"qagpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvwXSfc2ln9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3885031c-8c9f-4b18-f8d0-64587410636e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 7ae06a5af942cfd2e21de64ee697126eb8b0d0b9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQP40UCwE5CC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k00op41FlL-f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "\n",
        "import evaluate\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from evaluate import load\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ruORS1KlO66"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_excel(\"/content/drive/MyDrive/QA dataset talentsprint/aiml-qa-train.xlsx\")\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEfbZ5oYEQ05"
      },
      "outputs": [],
      "source": [
        "df_dev = pd.read_excel(\"/content/drive/MyDrive/QA dataset talentsprint/aiml-qa-dev.xlsx\")\n",
        "df_dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4AholQblRyS"
      },
      "outputs": [],
      "source": [
        "list_of_references = []\n",
        "for i in range(0, len(df)):\n",
        "    list_of_references.append([df.iloc[i, 1], df.iloc[i, 2]])\n",
        "print(len(list_of_references))\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur63wRdbD-n6"
      },
      "outputs": [],
      "source": [
        "question_word_count_list = []\n",
        "for sentence in df_train['question']:\n",
        "  question_word_count_list.append(len(sentence.split(\" \")))\n",
        "\n",
        "answer_word_count_list = []\n",
        "for sentence in df_train['answer']:\n",
        "  answer_word_count_list.append(len(sentence.split(\" \")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHM31eTPFi9r"
      },
      "outputs": [],
      "source": [
        "plt.hist(question_word_count_list, bins=10, color='blue')\n",
        "plt.title('Question word count')\n",
        "plt.xlabel('question length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K99_f6k5F2LF"
      },
      "outputs": [],
      "source": [
        "plt.hist(answer_word_count_list, bins=10, color='blue')\n",
        "plt.title('Answer word count')\n",
        "plt.xlabel('Answer length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C38S-CvPi1ow"
      },
      "outputs": [],
      "source": [
        "pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxZ6dgUJlXTj"
      },
      "outputs": [],
      "source": [
        "train_file_path = \"/content/drive/MyDrive/QA dataset talentsprint/question-answer-train.txt\"\n",
        "eval_file_path = \"/content/drive/MyDrive/QA dataset talentsprint/question-answer-dev.txt\"\n",
        "model_name = 'gpt2'\n",
        "rouge = evaluate.load('rouge')\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "#meteor = evaluate.load('meteor')\n",
        "output_dir = '/content/result2'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 4\n",
        "num_train_epochs = 20\n",
        "save_steps = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nvfNrUlcOo"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer):\n",
        "    dataset = LineByLineTextDataset(\n",
        "                tokenizer=tokenizer,\n",
        "                file_path=file_path,\n",
        "                block_size=512\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm\n",
        "    )\n",
        "\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    # TODO: Separate only the subject from string\n",
        "    # Ensure that for preds, you have a list of only the generated subject parts\n",
        "    # For labels, it should be a list of list of only the reference subjects\n",
        "    # NO OTHER CONTENT: EMAIL / SEPARATORS SHOULD BE OUTPUT AFTER POSTPROCESSING\n",
        "\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # print('logits:', logits.shape)\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # print('pred_ids:', pred_ids.shape)\n",
        "\n",
        "    return pred_ids, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    list_of_prediction = []\n",
        "    list_of_references = []\n",
        "    for i in range(0, len(df)):\n",
        "        try:\n",
        "#               print(i)\n",
        "#               print(list_of_references[i])\n",
        "              inputs = tokenizer(\"Question: \" + df.iloc[i, 0] + ' Answer: ', return_tensors=\"pt\")\n",
        "              inputs['input_ids'] = inputs['input_ids'].cpu()  # Move input tensor to CPU if necessary\n",
        "              device = torch.device(\"cuda:0\")  # Specify the CUDA device\n",
        "              model.to(device)  # Move the model to the CUDA device\n",
        "\n",
        "              # Move the input tensor to the CUDA device\n",
        "              inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "              outputs = model.generate(inputs['input_ids'], max_new_tokens=30, do_sample=True, top_k=30, top_p=0.95)\n",
        "              prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "              # Generate outputs using the model on the CUDA device\n",
        "              #print(prediction)\n",
        "              prediction = prediction.split('Answer: ')[1]\n",
        "              list_of_prediction.append(prediction)\n",
        "              list_of_references.append([df.iloc[i, 1], df.iloc[i, 2]])\n",
        "              i = i + 1\n",
        "#               print(prediction)\n",
        "        except Exception as e:\n",
        "              i = i + 1\n",
        "    result = rouge.compute(predictions=list_of_prediction, references=list_of_references)\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=list_of_prediction, references=list_of_references, lowercase = True)\n",
        "    results_bert = bertscore.compute(predictions=list_of_prediction, references=list_of_references, lang=\"en\")\n",
        "    #results_meteor = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    wandb.log({\n",
        "        \"R1\": round(result[\"rouge1\"], 4),\n",
        "        \"R2\": round(result[\"rouge2\"], 4),\n",
        "        \"RL\": round(result[\"rougeL\"], 4),\n",
        "        \"RLsum\": round(result[\"rougeLsum\"], 4),\n",
        "        \"bleu\": round(results_sacrebleu[\"score\"], 4),\n",
        "        \"precision1\": round(results_bert[\"precision\"][0], 4),\n",
        "        \"precision2\": round(results_bert[\"precision\"][1], 4),\n",
        "        \"recall1\": round(results_bert[\"recall\"][0], 4),\n",
        "        \"recall2\": round(results_bert[\"recall\"][1], 4),\n",
        "        \"f1-score1\": round(results_bert[\"f1\"][0], 4),\n",
        "        \"f1-score2\": round(results_bert[\"f1\"][1], 4)\n",
        "    })\n",
        "    return {\n",
        "        \"R1\": round(result[\"rouge1\"], 4),\n",
        "        \"R2\": round(result[\"rouge2\"], 4),\n",
        "        \"RL\": round(result[\"rougeL\"], 4),\n",
        "        \"RLsum\": round(result[\"rougeLsum\"], 4),\n",
        "        \"bleu\": round(results_sacrebleu[\"score\"], 4),\n",
        "        \"precision1\":round(results_bert[\"precision\"][0], 4),\n",
        "        \"precision2\":round(results_bert[\"precision\"][1], 4),\n",
        "        \"recall1\":round(results_bert[\"recall\"][0], 4),\n",
        "        \"recall2\":round(results_bert[\"recall\"][1], 4),\n",
        "        \"f1-score1\":round(results_bert[\"f1\"][0], 4),\n",
        "        \"f1-score2\":round(results_bert[\"f1\"][1], 4)\n",
        "    }\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "eval_dataset = load_dataset(eval_file_path, tokenizer)\n",
        "data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          evaluation_strategy = \"epoch\",\n",
        "#          eval_steps = 500,\n",
        "          learning_rate=1e-4,\n",
        "          save_strategy = \"steps\",\n",
        "          save_steps = 10000,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          per_device_eval_batch_size=1,\n",
        "          num_train_epochs=num_train_epochs\n",
        "      )\n",
        "\n",
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=eval_dataset,\n",
        "          preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "          compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}